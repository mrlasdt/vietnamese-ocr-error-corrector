SEED = 42
NGRAMS = 5  # use to construct dataset
THRESH = 0.99  # thresh the softmax probability of seq2seq model
# accented_chars_vietnamese = [
#     'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',
#     'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',
#     'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',
#     'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',
#     'í', 'ì', 'ỉ', 'ĩ', 'ị',
#     'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',
#     'đ',
# ]

# accented_chars_vietnamese.extend([c.upper() for c in accented_chars_vietnamese])
alphabet = list(
    "aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789! \"#$%&''()*+,-./:;<=>?@[\]^_{|}~")
# "aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789! \"#$%&'()*+,-./:;<=>?@[\]^_{|}~")  # remove duplicate '

# chars_regrex = '[aàảãáạăằẳẵắặâầẩẫấậAÀẢÃÁẠĂẰẲẴẮẶÂẦẨẪẤẬoòỏõóọôồổỗốộơờởỡớợOÒỎÕÓỌÔỒỔỖỐỘƠỜỞỠỚỢeèẻẽéẹêềểễếệEÈẺẼÉẸÊỀỂỄẾỆuùủũúụưừửữứựUÙỦŨÚỤƯỪỬỮỨỰiìỉĩíịIÌỈĨÍỊyỳỷỹýỵYỲỶỸÝỴnNvVmMCG]'
# same_chars = {
#     'a': ['á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ'],  # add â, ă
#     'A': ['Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ'],
#     'O': ['Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'Q'],
#     'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'q'],
#     'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ê'],
#     'E': ['É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ê'],
#     'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ư'],
#     'U': ['Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Ư'],
#     'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị'],
#     'I': ['Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị'],
#     'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'v'],
#     'Y': ['Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'V'],
#     'n': ['m'],
#     'N': ['M'],
#     'v': ['y'],
#     'V': ['Y'],
#     'm': ['n'],
#     'M': ['N'],
#     'C': ['G'],
#     'G': ['C']
# }
same_chars = {  # base on error analysis
    'a': ['ầ', 'a', 'ă', 'ắ', 'ẩ', 'à', 'ấ', 'â', 'ạ', 'ằ', 'ẳ', 'á', 'ả', 'ẵ', 'a'],  # add â, ă
    'A': ['Ấ', 'Ẩ', 'Á', 'Ả', 'A'],
    'o': ['ở', 'ổ', 'ỡ', 'ơ', 'õ', 'ò', 'ọ', 'ợ', 'ỏ', 'ồ', 'ô', 'o', 'ố', 'ỗ', 'o'],
    'e': ['è', 'é', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'e'],
    'E': ['É', 'E'],
    'u': ['ứ', 'ư', 'ử', 'u'],
    'U': ['Ú', 'Ủ', 'U'],
    'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị', 'i'],
    'I': ['Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'I'],
    'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'y'],
}
chars_regrex = '[' + ''.join(set(sum(same_chars.values(), []) + list(same_chars.keys()))) + ']'


most_wrong_accent_chars = {  # base on error analysis
    'ằ': ['ẳ'],
    'ể': ['ề', 'ế'],
    'ă': ['ắ', 'ằ'],
    'ổ': ['ố', 'ô', 'ỗ'],
    'ề': ['ể', 'ê'],
    'e': ['é'],
    'ô': ['ồ', 'ổ', 'ố'],
    'i': ['í'],
    'ò': ['ỏ'],
    'à': ['ả'],
    'ê': ['ề'],
    'â': ['ầ', 'ấ'],
}
dist = [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
assert int(sum(dist)) == 1
most_wrong_distribution = {k: v for k, v in zip(most_wrong_accent_chars.keys(), dist)}

# print(most_wrong_distribution)
most_wrong_chars_regrex = '[' + ''.join(list(most_wrong_accent_chars.keys())) + ']'
most_wrong_chance = 0.8
# indices = [alphabet.index(c) + 4 for c in accented_chars_vietnamese]
# vocab_size = len(alphabet)
# litle_char = list('"#$%&''()*+,-./:;<=>?@[\]^_{|}~')

no_space_char = list("!#$%''*+,./:;?^_|~")


# longest_common_subsequence(common_randomcase, string)
